{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 4 - Robustness of DNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On Assignment 1, we saw that DNNs are very good at learning some tasks such as MNIST digit classification. At the same time, we saw the impact of data bias towards the output classification. Here, we will see an inherent flaw in the way we do inference on the trained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import itertools\n",
    "import torchvision\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch import nn, optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "flatten = itertools.chain.from_iterable\n",
    "\n",
    "# Some helper functions\n",
    "\n",
    "def plot_loss(loss_as_list):\n",
    "    \"\"\"\n",
    "    Plot the loss curve from a list of loss terms.\n",
    "    \"\"\"\n",
    "    plt.plot(loss_as_list, 'k')\n",
    "    _ = plt.title(\"Loss Curve\")\n",
    "    _ = plt.xlabel(\"Epochs\")\n",
    "    _ = plt.ylabel(\"Loss\")\n",
    "    \n",
    "def get_classification_results(model, loader):\n",
    "    \"\"\"\n",
    "    Print the accuracy of a trained model.\n",
    "    Loss: Cross Entropy\n",
    "    \"\"\"\n",
    "    correct, total = 0, 0\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "\n",
    "    for xs, ts in test_loader:\n",
    "        xs = xs.view(-1, 784) # flatten the image\n",
    "        zs = model(xs) # do forward pass\n",
    "        pred = zs.max(1, keepdim=True)[1] # get the index of the max logit\n",
    "        correct += pred.eq(ts.view_as(pred)).sum().item() # count equal values\n",
    "        total += int(ts.shape[0]) # get total values\n",
    "\n",
    "        predictions.append(pred)\n",
    "        true_labels.append(ts)\n",
    "\n",
    "    accuracy = correct / total\n",
    "    conf_matrix = confusion_matrix(list(flatten(true_labels)), list(flatten(predictions)))\n",
    "    cl_report = classification_report(list(flatten(true_labels)), list(flatten(predictions)), digits=4)\n",
    "\n",
    "    print(cl_report)\n",
    "    print(conf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load original MNIST data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(13)\n",
    "\n",
    "N_train = 64\n",
    "N_test = 256\n",
    "\n",
    "# We will use torch.utils.data.DataLoader to wrap our dataset.\n",
    "# This provides easier batching, GPU support, etc.\n",
    "# Calling torchvision.datasets.MNIST() will download and format the MNIST\n",
    "# dataset with the transforms we specify. Here, in the transforms we first convert\n",
    "# the image to PyTorch tensor, and then normalize the image based on a given mean\n",
    "# and standard deviation. Normalizing the image does: image = (image - mean) / std.\n",
    "# We shuffle the data as well by defining shuffle=True.\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "  torchvision.datasets.MNIST('../Datasets/', train=True, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ])),\n",
    "  batch_size=N_train, shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "  torchvision.datasets.MNIST('../Datasets/', train=False, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ])),\n",
    "  batch_size=N_test, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the model and hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 784\n",
    "hidden_sizes = [128, 64]\n",
    "output_size = 10\n",
    "\n",
    "# Hyper Parameters\n",
    "lr = 0.003 # learning rate\n",
    "NUM_EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MLP():\n",
    "    \"\"\"\n",
    "    A function implementation of the model definition.\n",
    "    \"\"\"\n",
    "    model = nn.Sequential(nn.Linear(input_size, hidden_sizes[0]),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(hidden_sizes[0], hidden_sizes[1]),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(hidden_sizes[1], output_size)\n",
    "                     )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, NUM_EPOCHS, train_loader):\n",
    "    \"\"\"\n",
    "    A function to train the neural network model.\n",
    "    \"\"\"\n",
    "    loss_fn = nn.CrossEntropyLoss() # also called criterion sometimes.\n",
    "\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "    start = time.time()\n",
    "    loss_as_list = []\n",
    "\n",
    "    for EPOCH in range(NUM_EPOCHS):\n",
    "        running_loss = 0\n",
    "        for images, labels in train_loader:\n",
    "            # Flatten MNIST images into a 784 long vector\n",
    "            images = images.view(images.shape[0], -1)\n",
    "\n",
    "            # Training pass\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            output = model(images)\n",
    "            loss = loss_fn(output, labels)\n",
    "            loss_as_list.append(loss)\n",
    "\n",
    "            #This is where the model learns by backpropagating\n",
    "            loss.backward()\n",
    "\n",
    "            #And optimizes its weights here\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "        else:\n",
    "            print(\"Epoch {} - Training loss: {}\".format(EPOCH, running_loss/len(train_loader)))\n",
    "\n",
    "    print(\"\\nTraining Time (in minutes) =\",(time.time()-start)/60)\n",
    "    return(loss_as_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the trained DNN model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We shall try to load the trained DNN on the MNIST data from Assignment 1. If the file doesn't exist, we will train the network again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(\"../Assignment-1/mnist_original.pt\"):\n",
    "    model = torch.load(\"../Assignment-1/mnist_original.pt\")\n",
    "else:\n",
    "    # Define the model\n",
    "    model = MLP()\n",
    "\n",
    "    # Start training the model on the train_loader.\n",
    "    loss_values = train(model, NUM_EPOCHS, train_loader)\n",
    "    \n",
    "    plot_loss(loss_values)\n",
    "    \n",
    "    get_classification_results(model, test_loader)\n",
    "    \n",
    "    # Save the model\n",
    "    torch.save(model, 'mnist_original.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Study the representations\n",
    "\n",
    "# Extract the hidden layer latent dense representations to study the distribution of the representations.\n",
    "# We can select a specific layer for our study.\n",
    "# Let us extract the penultimate layer as the embedding layer. Hence we use `-2` index since `:` means upto but not including.\n",
    "\n",
    "embd_model = nn.Sequential(*list(model.children()))[:-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that our new model don't have the output layer and associated ReLU activation layer.\n",
    "embd_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can define a z vector and append all activations to that.\n",
    "zs = []\n",
    "for xs, ts in test_loader:\n",
    "    xs = xs.view(-1, 784) # flatten the image\n",
    "    zs.append(embd_model(xs).detach().numpy()) # do forward pass to extract embeddings and append to zs.\n",
    "\n",
    "zs = np.vstack(zs) # Stack all the embeddings. This will give you 10000*64 array, since embedding size (out_features of (2) Linear) is 64.\n",
    "\n",
    "zs_mean = np.average(zs, axis=0) # Find mean of all embeddings with respect to depth axis (axis=0). This will give you 1*64 vector.\n",
    "\n",
    "zs_std = np.std(zs, axis=0) # Find the standard deviation all embeddings with respect to the depth axis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize some resutls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_subset = enumerate(test_loader)\n",
    "batch_idx, (one_batch_of_test_subset_x, one_batch_of_test_subset_y) = next(test_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "plt.imshow(one_batch_of_test_subset_x[i][0], cmap='gray', interpolation='none')\n",
    "_ = plt.title(\"Ground Truth: {}\".format(one_batch_of_test_subset_y[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the output logit\n",
    "\n",
    "output_logits = model(one_batch_of_test_subset_x[i][0].view(-1, 784))\n",
    "print(output_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If required you can convert the tensor to a numpy array by calling detach() and numpy()\n",
    "output_logits.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the maximum logit value\n",
    "output_logits.max(1, keepdim=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the maximum value and it's location corresponds well to output position we were looking for! How can we extract this using code?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = output_logits.max(1, keepdim=True)[1]\n",
    "print(\"Predicted Label:\", pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_label = one_batch_of_test_subset_y[i].view_as(pred).sum().item()\n",
    "print(\"True Label:\", true_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! We have a way to do model inference on new inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feed random gaussian noise as input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of feeding an actual MNIST image, what happens if we feed a noisy image, randomly sampled from a gaussian distribution? Would our model know whether it is just noise that we are feeding it? Would it complain that it cannot find a proper answer? Let's find out..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample a gaussian noise vector from a normal distribution\n",
    "mu, sigma = 0, 0.2\n",
    "gaussian_noise = np.random.normal(mu, sigma, 784)\n",
    "\n",
    "# Plot the noise vector as an image\n",
    "plt.imshow(gaussian_noise.reshape(28,28), cmap='gray', interpolation='none')\n",
    "_ = plt.title(\"Ground Truth: NONE\")\n",
    "\n",
    "# Convert the noise vector to a tensor\n",
    "gaussian_noise = torch.tensor(gaussian_noise, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_logits_noise = model(gaussian_noise.view(-1, 784))\n",
    "print(output_logits_noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_noise = output_logits_noise.max(1, keepdim=True)[1]\n",
    "print(\"Predicted Label:\", pred_noise)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model we trained predicted a $3$ for the gaussian noise. Isn't this an unfavorable outcome? What if we were to use this model in a scenario where the decisions are mission-critical. We need to think about such out-of-distribution data as we move forward!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How different are the activations?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can extract the embeddings using the same method we used in Assignment 1. Let's extract the embedding for the gaussian noise input and compare with the mean of the clean embeddings we found earlier in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zs_noise = embd_model(gaussian_noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(zs_mean, 'k', label='Mean of clean embeddings')\n",
    "plt.plot(zs_noise.detach().numpy(), 'r', label='Noise Input')\n",
    "plt.xlabel('Element location within embedding')\n",
    "plt.ylabel('Embedding value')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is your observation?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
