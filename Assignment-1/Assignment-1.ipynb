{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This assignment has two sections:\n",
    "\n",
    "    a) Study of hyperparameter impact - after you complete the program, you should add one page discussion and conclusion regarding hyperparameter impact to your notebook.\n",
    "    b) Study of data bias impact - after you complete the program, you should add one page discussion and conclusion regarding data bias impact to your notebook.\n",
    "### Model: \n",
    "![alt text](model.png \"Architecture\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import itertools\n",
    "import torchvision\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch import nn, optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "flatten = itertools.chain.from_iterable\n",
    "\n",
    "# Some helper functions\n",
    "\n",
    "def plot_loss(loss_as_list):\n",
    "    \"\"\"\n",
    "    Plot the loss curve from a list of loss terms.\n",
    "    \"\"\"\n",
    "    plt.plot(loss_as_list, 'k')\n",
    "    _ = plt.title(\"Loss Curve\")\n",
    "    _ = plt.xlabel(\"Epochs\")\n",
    "    _ = plt.ylabel(\"Loss\")\n",
    "    \n",
    "def get_classification_results(model, loader):\n",
    "    \"\"\"\n",
    "    Print the accuracy of a trained model.\n",
    "    Loss: Cross Entropy\n",
    "    \"\"\"\n",
    "    correct, total = 0, 0\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "\n",
    "    for xs, ts in test_loader:\n",
    "        xs = xs.view(-1, 784) # flatten the image\n",
    "        zs = model(xs) # do forward pass\n",
    "        pred = zs.max(1, keepdim=True)[1] # get the index of the max logit\n",
    "        correct += pred.eq(ts.view_as(pred)).sum().item() # count equal values\n",
    "        total += int(ts.shape[0]) # get total values\n",
    "\n",
    "        predictions.append(pred)\n",
    "        true_labels.append(ts)\n",
    "\n",
    "    accuracy = correct / total\n",
    "    conf_matrix = confusion_matrix(list(flatten(true_labels)), list(flatten(predictions)))\n",
    "    cl_report = classification_report(list(flatten(true_labels)), list(flatten(predictions)), digits=4)\n",
    "\n",
    "    print(cl_report)\n",
    "    print(conf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the past three lectures we saw how to load data, define multi-layer perceptron (MLP) model, and train them. In this assignment, we will learn nuances of the dataset and some inherent limitations of deep neural networks. Some helper code that is required is provided and students are encouraged to read documentations and come up with code to carry out some operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a) Impact of Hyperparameters in Learning Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameters are parameters that are set before the learning process and influence the learning performance of your algorithm. They are tunable parameters and influence how the model handles different aspects of forward and backward passes during optimization. For example, as we saw, changing the learning rate ($\\eta$) could improve or degrade the learning performance. This happens due to unstable training for large values of $\\eta$ or small changes in weight updates due to very small values of $\\eta$.\n",
    "\n",
    "Let us look at some hyper parameters and understand how they impact the learning performance. First we will define the hyperparameters and train the Deep Neural Network (DNN) model we discussed in Lecture 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load original MNIST data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(13)\n",
    "\n",
    "N_train = 64\n",
    "N_test = 256\n",
    "\n",
    "# We will use torch.utils.data.DataLoader to wrap our dataset.\n",
    "# This provides easier batching, GPU support, etc.\n",
    "# Calling torchvision.datasets.MNIST() will download and format the MNIST\n",
    "# dataset with the transforms we specify. Here, in the transforms we first convert\n",
    "# the image to PyTorch tensor, and then normalize the image based on a given mean\n",
    "# and standard deviation. Normalizing the image does: image = (image - mean) / std.\n",
    "# We shuffle the data as well by defining shuffle=True.\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "  torchvision.datasets.MNIST('../Datasets/', train=True, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ])),\n",
    "  batch_size=N_train, shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "  torchvision.datasets.MNIST('../Datasets/', train=False, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ])),\n",
    "  batch_size=N_test, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the model and hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 784\n",
    "hidden_sizes = [128, 64]\n",
    "output_size = 10\n",
    "\n",
    "# Hyper Parameters\n",
    "lr = 0.003 # learning rate\n",
    "NUM_EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MLP():\n",
    "    \"\"\"\n",
    "    A function implementation of the model definition.\n",
    "    \"\"\"\n",
    "    model = nn.Sequential(nn.Linear(input_size, hidden_sizes[0]),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(hidden_sizes[0], hidden_sizes[1]),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(hidden_sizes[1], output_size)\n",
    "                     )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, NUM_EPOCHS, train_loader):\n",
    "    \"\"\"\n",
    "    A function to train the neural network model.\n",
    "    \"\"\"\n",
    "    loss_fn = nn.CrossEntropyLoss() # also called criterion sometimes.\n",
    "\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "    start = time.time()\n",
    "    loss_as_list = []\n",
    "\n",
    "    for EPOCH in range(NUM_EPOCHS):\n",
    "        running_loss = 0\n",
    "        for images, labels in train_loader:\n",
    "            # Flatten MNIST images into a 784 long vector\n",
    "            images = images.view(images.shape[0], -1)\n",
    "\n",
    "            # Training pass\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            output = model(images)\n",
    "            loss = loss_fn(output, labels)\n",
    "            loss_as_list.append(loss)\n",
    "\n",
    "            #This is where the model learns by backpropagating\n",
    "            loss.backward()\n",
    "\n",
    "            #And optimizes its weights here\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "        else:\n",
    "            print(\"Epoch {} - Training loss: {}\".format(EPOCH, running_loss/len(train_loader)))\n",
    "\n",
    "    print(\"\\nTraining Time (in minutes) =\",(time.time()-start)/60)\n",
    "    return(loss_as_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "model = MLP()\n",
    "\n",
    "# Start training the model on the train_loader.\n",
    "loss_values = train(model, NUM_EPOCHS, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the loss curve\n",
    "plot_loss(loss_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function we defined in the beginning generates a classification report based\n",
    "# on https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html#sklearn.metrics.classification_report\n",
    "# We also generate a confusion matrix to study the misclassifications.\n",
    "get_classification_results(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "torch.save(model, 'mnist_original.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Study the representations\n",
    "\n",
    "# Extract the hidden layer latent dense representations to study the distribution of the representations.\n",
    "# We can select a specific layer for our study.\n",
    "# Let us extract the penultimate layer as the embedding layer. Hence we use `-2` index since `:` means upto but not including.\n",
    "\n",
    "embd_model = nn.Sequential(*list(model.children()))[:-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that our new model don't have the output layer and associated ReLU activation layer.\n",
    "embd_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can define a z vector and append all activations to that.\n",
    "zs = []\n",
    "for xs, ts in test_loader:\n",
    "    xs = xs.view(-1, 784) # flatten the image\n",
    "    zs.append(embd_model(xs).detach().numpy()) # do forward pass to extract embeddings and append to zs.\n",
    "\n",
    "zs = np.vstack(zs) # Stack all the embeddings. This will give you 10000*64 array, since embedding size (out_features of (2) Linear) is 64.\n",
    "\n",
    "zs_mean = np.average(zs, axis=0) # Find mean of all embeddings with respect to depth axis (axis=0). This will give you 1*64 vector.\n",
    "\n",
    "zs_std = np.std(zs, axis=0) # Find the standard deviation all embeddings with respect to the depth axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the covariance of embeddings.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change Learning Rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Our model is generating 97.71% accuracy. What happens when we change the learning rate to $\\eta = 0.05$? Please add your code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define new hyper parameters here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "model_lr = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training the model on the train_loader.\n",
    "model_lr_loss_values = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the loss curve\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "torch.save(model_lr, 'mnist_change_lr.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Study the representations\n",
    "\n",
    "# Let us extract the penultimate layer as the embedding layer.\n",
    "\n",
    "embd_model_lr = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that our new model don't have the output layer and associated ReLU activation layer.\n",
    "embd_model_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can define a z vector and append all activations to that.\n",
    "zs_embd_model_lr = []\n",
    "for xs, ts in test_loader:\n",
    "    xs = xs.view(-1, 784) # flatten the image\n",
    "    zs_embd_model_lr.append(embd_model_lr(xs).detach().numpy()) # do forward pass to extract embeddings and append to zs.\n",
    "\n",
    "zs_embd_model_lr = ... # Stack all the embeddings. \n",
    "\n",
    "zs_embd_model_lr_mean = ... # Find mean of all embeddings with respect to depth axis (axis=0).\n",
    "\n",
    "zs_embd_model_lr_std = ... # Find the standard deviation all embeddings with respect to the depth axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the covariance of embeddings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are your findings? Write below. (Make sure you change the type of the cell to MarkDown)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change Number of Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens if we increase the number of layers in the neural network? Can we expect higher accuracy? Create three hidden layers instead of two. The third hidden layer size can be 32."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 784\n",
    "hidden_sizes_bigger_MLP = [] #\n",
    "output_size = 10\n",
    "\n",
    "def bigger_MLP():\n",
    "    \"\"\"\n",
    "    A function implementation of the model definition.\n",
    "    \"\"\"\n",
    "    model = nn.Sequential(\n",
    "                         # Fill your network here\n",
    "                         )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "bigger_model = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training the model on the train_loader.\n",
    "bigger_model_loss_values = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the loss curve\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "torch.save(bigger_model, 'mnist_bigger_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Study the representations\n",
    "\n",
    "# Let us extract the penultimate layer as the embedding layer.\n",
    "\n",
    "embd_bigger_model = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that our new model don't have the output layer and associated ReLU activation layer.\n",
    "embd_bigger_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can define a z vector and append all activations to that.\n",
    "zs_embd_bigger_model = []\n",
    "for xs, ts in test_loader:\n",
    "    xs = xs.view(-1, 784) # flatten the image\n",
    "    zs_embd_bigger_model.append(embd_bigger_model(xs).detach().numpy()) # do forward pass to extract embeddings and append to zs.\n",
    "\n",
    "zs_embd_bigger_model = ... # Stack all the embeddings. \n",
    "\n",
    "zs_embd_bigger_model_mean = ... # Find mean of all embeddings with respect to depth axis (axis=0).\n",
    "\n",
    "zs_embd_bigger_model_std = ... # Find the standard deviation all embeddings with respect to the depth axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the covariance of embeddings.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion and Conclusion Section - Major Finding (1 Page)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What poked your interest? Did you find any changes in the learning performance and representations when you changed the hyper parameters or increased the size of the model? Write your findings below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b) Data Bias due to Class Imbalance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MNIST is a relatively clean dataset. There are 60,000 total training samples and each class has roughly 6000 samples each. Roughly. But there is not a lot of changes. Infact, here's proof."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'transforms' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-8c5aebfb140e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Transform to normalized Tensors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m transform = transforms.Compose([transforms.ToTensor(),\n\u001b[0m\u001b[1;32m      3\u001b[0m                                 transforms.Normalize((0.1307,), (0.3081,))])\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMNIST\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../Datasets/'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'transforms' is not defined"
     ]
    }
   ],
   "source": [
    "# Transform to normalized Tensors \n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.1307,), (0.3081,))])\n",
    "\n",
    "train_dataset = datasets.MNIST('../Datasets/', train=True, transform=transform, download=True)\n",
    "test_dataset = datasets.MNIST('../Datasets/', train=False, transform=transform, download=True)\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=len(train_dataset))\n",
    "test_loader = DataLoader(test_dataset, batch_size=len(test_dataset))\n",
    "\n",
    "train_dataset_array = next(iter(train_loader))[0].numpy()\n",
    "train_dataset_array_labels = next(iter(train_loader))[1].numpy()\n",
    "\n",
    "test_dataset_array = next(iter(test_loader))[0].numpy()\n",
    "test_dataset_array_labels = next(iter(test_loader))[1].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_vals = np.unique(train_dataset_array_labels, return_counts=True)\n",
    "\n",
    "plt.bar(unique_vals[0], unique_vals[1], color='gray')\n",
    "plt.title(\"MNIST Training Data Distribution\")\n",
    "plt.xlabel(\"Class\")\n",
    "plt.ylabel(\"Num Samples\")\n",
    "_ = plt.xticks(unique_vals[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there is not much difference in terms of number of samples per class. Now, what happens if there is considerably less sample in one class? Below is some code to reduce the number of samples from class `1`.  Train, test, and study the embeddings of a model on the biased dataset and compare with the previous study."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "max_count = 6500\n",
    "pos2del = []\n",
    "label2del = 1\n",
    "for i in range(len(train_dataset_array_labels)):\n",
    "    if train_dataset_array_labels[i] == label2del and count < max_count:\n",
    "        count += 1\n",
    "        pos2del.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_biased_X = np.delete(train_dataset_array, pos2del, axis=0)\n",
    "train_biased_Y = np.delete(train_dataset_array_labels, pos2del, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_train = 64\n",
    "N_test = 256\n",
    "\n",
    "t_mnist_assn_1a_train_X = torch.Tensor(train_biased_X)\n",
    "t_mnist_assn_1a_train_Y = torch.Tensor(train_biased_Y).type(torch.LongTensor)\n",
    "    \n",
    "train_data = TensorDataset(t_mnist_assn_1a_train_X, t_mnist_assn_1a_train_Y)\n",
    "train_loader_bias = DataLoader(train_data, batch_size=N_train, shuffle=True)\n",
    "\n",
    "# Note that we didn't do any operations on the test loader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_vals = np.unique(train_biased_Y, return_counts=True)\n",
    "\n",
    "plt.bar(unique_vals[0], unique_vals[1], color='gray')\n",
    "plt.title(\"MNIST Training Data Distribution - Class 1 Imbalance\")\n",
    "plt.xlabel(\"Class\")\n",
    "plt.ylabel(\"Num Samples\")\n",
    "_ = plt.xticks(unique_vals[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 784\n",
    "hidden_sizes = [128, 64]\n",
    "output_size = 10\n",
    "\n",
    "# Hyper Parameters\n",
    "lr = 0.003 # learning rate\n",
    "NUM_EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "model_biased_1 = MLP()\n",
    "\n",
    "# Start training the model on the train_loader_bias.\n",
    "loss_values_model_biased_1 = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the loss curve\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "torch.save(model_biased_1, 'mnist_biased_1.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Study the representations\n",
    "\n",
    "# Let us extract the penultimate layer as the embedding layer.\n",
    "\n",
    "embd_model_biased_1 = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that our new model don't have the output layer and associated ReLU activation layer.\n",
    "embd_model_biased_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can define a z vector and append all activations to that.\n",
    "zs_embd_model_biased_1 = []\n",
    "for xs, ts in test_loader:\n",
    "    xs = xs.view(-1, 784) # flatten the image\n",
    "    zs_embd_model_biased_1.append(embd_model_biased_1(xs).detach().numpy()) # do forward pass to extract embeddings and append to zs.\n",
    "\n",
    "zs_embd_model_biased_1 = ... # Stack all the embeddings. \n",
    "\n",
    "zs_embd_model_biased_1_mean = ... # Find mean of all embeddings with respect to depth axis (axis=0).\n",
    "\n",
    "zs_embd_model_biased_1_std = ... # Find the standard deviation all embeddings with respect to the depth axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the covariance of embeddings.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion and Conclusion Section - Major Finding (1 Page)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
