{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 2 - Basic Neural Network Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now upto speed with the various APIs in PyTorch that enables automatic differentiation and weight updates. We can apply the things we learned on a real dataset now. Let us see how we can train a neural network to classify handwritten digits using the MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch import nn, optim\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Torch has a sister package called `TorchVision` which includes a variety of datasets and `torch.utils` has a `DataLoader` method to aid our research. We can specify image transformations directly in the `DataLoader` without the need to apply augmentations to individual samples before training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MNIST is a dataset with $28\\times 28$ images of handwritten digits. There are a total of 70,000 images. 60k for training and 10k for testing. We shall use `torchvision.dataset` to load MNIST dataset to memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(13)\n",
    "\n",
    "N_train = 64\n",
    "N_test = 256\n",
    "\n",
    "# We will use torch.utils.data.DataLoader to wrap our dataset.\n",
    "# This provides easier batching, GPU support, etc.\n",
    "# Calling torchvision.datasets.MNIST() will download and format the MNIST\n",
    "# dataset with the transforms we specify. Here, in the transforms we first convert\n",
    "# the image to PyTorch tensor, and then normalize the image based on a given mean\n",
    "# and standard deviation. Normalizing the image does: image = (image - mean) / std.\n",
    "# We shuffle the data as well by defining shuffle=True.\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "  torchvision.datasets.MNIST('../Datasets/', train=True, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ])),\n",
    "  batch_size=N_train, shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "  torchvision.datasets.MNIST('../Datasets/', train=False, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ])),\n",
    "  batch_size=N_test, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at a small subset of the test dataset to get familiarized with the MNIST dataset. We can use `enumerate` method in Python to loop through lists or arrays. The `Dataloader` also support `enumerate` function. We can call `next` on the subset to get one batch of data out. Recall that we defined our testing batch size `N_test=256`. So we expect 256 number of $1\\times 28 \\times28$ tensors, since PyTorch save data by default in $NCHW$ format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_subset = enumerate(test_loader)\n",
    "batch_idx, (one_batch_of_test_subset_x, one_batch_of_test_subset_y) = next(test_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_batch_of_test_subset_x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use `matplotlib` to plot some of the images and corresponding labels inside the test data subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "for i in range(6):\n",
    "    plt.subplot(2,3,i+1)\n",
    "    plt.tight_layout()\n",
    "    plt.imshow(one_batch_of_test_subset_x[i][0], cmap='gray', interpolation='none')\n",
    "    plt.title(\"Ground Truth: {}\".format(one_batch_of_test_subset_y[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def view_classify(img, ps, cmap='gray'):\n",
    "    ''' \n",
    "    Function for viewing an image and it's predicted classes.\n",
    "    Source: https://github.com/amitrajitbose/handwritten-digit-recognition\n",
    "    '''\n",
    "    ps = ps.data.numpy().squeeze()\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(figsize=(6,9), ncols=2)\n",
    "    ax1.imshow(img.resize_(1, 28, 28).numpy().squeeze(), cmap=cmap)\n",
    "    ax1.axis('off')\n",
    "    ax2.barh(np.arange(10), ps)\n",
    "    ax2.set_aspect(0.1)\n",
    "    ax2.set_yticks(np.arange(10))\n",
    "    ax2.set_yticklabels(np.arange(10))\n",
    "    ax2.set_title('Class Probability')\n",
    "    ax2.set_xlim(0, 1.1)\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now build a fully-connected network with 4 layers (including the input layer - which we often don't mention in PyTorch). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 784\n",
    "hidden_sizes = [128, 64]\n",
    "output_size = 10\n",
    "\n",
    "model = nn.Sequential(nn.Linear(input_size, hidden_sizes[0]),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(hidden_sizes[0], hidden_sizes[1]),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(hidden_sizes[1], output_size),\n",
    "                      nn.LogSoftmax(dim=1))\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will define the loss function as Negative Log Likelihood (https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html).\n",
    "\n",
    "We will use Stochastic Gradient Descend (SGD) as the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.NLLLoss() # also called criterion sometimes.\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.003, momentum=0.9)\n",
    "start = time.time()\n",
    "\n",
    "NUM_EPOCHS = 5\n",
    "for EPOCH in range(NUM_EPOCHS):\n",
    "    running_loss = 0\n",
    "    for images, labels in train_loader:\n",
    "        # Flatten MNIST images into a 784 long vector\n",
    "        images = images.view(images.shape[0], -1)\n",
    "    \n",
    "        # Training pass\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(images)\n",
    "        loss = loss_fn(output, labels)\n",
    "        \n",
    "        #This is where the model learns by backpropagating\n",
    "        loss.backward()\n",
    "        \n",
    "        #And optimizes its weights here\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    else:\n",
    "        print(\"Epoch {} - Training loss: {}\".format(EPOCH, running_loss/len(train_loader)))\n",
    "print(\"\\nTraining Time (in minutes) =\",(time.time()-start)/60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = next(iter(test_loader))\n",
    "\n",
    "img = images[0].view(1, 784)\n",
    "with torch.no_grad():\n",
    "    logps = model(img)\n",
    "\n",
    "ps = torch.exp(logps)\n",
    "probab = list(ps.numpy()[0])\n",
    "print(\"Predicted Digit =\", probab.index(max(probab)))\n",
    "view_classify(img.view(1, 28, 28), ps, cmap='gray_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_count, all_count = 0, 0\n",
    "for images,labels in test_loader:\n",
    "    for i in range(len(labels)):\n",
    "        img = images[i].view(1, 784)\n",
    "        # Turn off gradients to speed up this part\n",
    "        with torch.no_grad():\n",
    "            logps = model(img)\n",
    "\n",
    "        # Output of the network are log-probabilities, need to take exponential for probabilities\n",
    "        ps = torch.exp(logps)\n",
    "        probab = list(ps.numpy()[0])\n",
    "        pred_label = probab.index(max(probab))\n",
    "        true_label = labels.numpy()[i]\n",
    "        if(true_label == pred_label):\n",
    "            correct_count += 1\n",
    "        all_count += 1\n",
    "\n",
    "print(\"Number Of Images Tested =\", all_count)\n",
    "print(\"\\nModel Accuracy =\", (correct_count/all_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving and Loading trained models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can save the whole model by using `torch.save`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'mnist.pytorch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use `torch.load` to load the model back. Let us use another variable to load the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_loaded = torch.load('mnist.pytorch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_count, all_count = 0, 0\n",
    "for images,labels in test_loader:\n",
    "    for i in range(len(labels)):\n",
    "        img = images[i].view(1, 784)\n",
    "        # Turn off gradients to speed up this part\n",
    "        with torch.no_grad():\n",
    "            logps = model_loaded(img)\n",
    "\n",
    "        # Output of the network are log-probabilities, need to take exponential for probabilities\n",
    "        ps = torch.exp(logps)\n",
    "        probab = list(ps.numpy()[0])\n",
    "        pred_label = probab.index(max(probab))\n",
    "        true_label = labels.numpy()[i]\n",
    "        if(true_label == pred_label):\n",
    "            correct_count += 1\n",
    "        all_count += 1\n",
    "\n",
    "print(\"Number Of Images Tested =\", all_count)\n",
    "print(\"\\nModel Accuracy =\", (correct_count/all_count))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
