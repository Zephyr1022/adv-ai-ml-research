{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 2 - Basic Neural Network APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will focus on building a small two-layer neural network using PyTorch. We will follow the example in PyTorch documentation, written by Justin Johnson, and discuss everything in detail: https://pytorch.org/tutorials/beginner/pytorch_with_examples.html. All code is based on this website. I will describe the code in detail and give comments as necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will mimic the input shape of MNIST dataset, where each image is $28\\times28$ in size. Before using real datasets, we will learn the PyTorch APIs using randomly generated data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every major deep learning platform is built to support either the $NCHW$ or $NHCW$ format, where $N$ is the batch size, $C$ is the number of channels, $H$ is the height, and $W$ is the width of the data. This is easier to relate with images. For example, a black and white square image of side 10 would have $H=10, W=10$ and $C=1$. If we have 20 such images in each batch of data, $N=20$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Layer Perceptron (MLP) or Fully-Connected Network\n",
    "### Pure Numpy Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us start with the pure Numpy version of a two layer neural network training. We're going to use dense/fully-connected layers in this exercise and create random data with the MNIST dimensions $28\\times28$. So our flattened input dimensions, $D_{in} = 28*28 = 784$. For MNIST, we have 10 outputs. So $D_{out} = 10$. Let us consider one hidden layer with 100 neurons, $H=100$. Also, recall that neural networks optimize the parameters using backpropagation. Using pure Numpy, we have to do forward and backpropagation while defining the derivatives ourselves. We saw that torch has `autograd`. We'll look into it in a bit. For now, pure Numpy..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img alt=\"2 Layer Neural Network\" src=\"images/nn2layer.jpg\" width=600>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N, D_in, H, D_out = 64, 784, 100, 10\n",
    "\n",
    "# Create random input and output data simulating the MNIST dimensions.\n",
    "# Note that the first dimension is the batch size \n",
    "# Note that D_in is 784, simulating the MNIST input dimensions flattened.\n",
    "# Note that D_out is 10, for the 0-9 digits.\n",
    "x = np.random.randn(N, D_in) \n",
    "y = np.random.randn(N, D_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have to create two weight matrices: one between input and hidden layers, \n",
    "# and another between hidden and output layers.\n",
    "\n",
    "# We will randomly initialize weights, but there are many other weight initializations\n",
    "# commonly used in practice.\n",
    "w1 = np.random.randn(D_in, H)\n",
    "w2 = np.random.randn(H, D_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(w1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(w2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the parameters of the neural network that we are trying to optimize."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will define the learning rate at which we do our optimizations. In the future we will see that learning rate can be actively changed during training based on the validation loss or other criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have to iterate through the data and change the weights according to the predictions that we get."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic algorithm:\n",
    "\n",
    "    While epoch not final_epoch:\n",
    "    \n",
    "        1. Fetch input x.\n",
    "\n",
    "        2. Do forward pass.\n",
    "        a) This means you pass x to h, and then h to output.\n",
    "        b) Value generated at output will be the predictions y_hat in code.\n",
    "\n",
    "        3. Find the L2 (squared sum) loss based on y and y_hat. \n",
    "        Final loss is \n",
    "        \n",
    "$\\mathbb{L} = \\sum_{n=1}^{N} \\left \\|\\tilde{y}_n - y_n\\right \\|^2_2$.\n",
    "\n",
    "        4. Compute the gradients for backpropagation. \n",
    "        Here, gradient is based on the final loss function, generating \n",
    "$2\\times (\\tilde{y}_n - y_n)$.\n",
    "\n",
    "        5. Multiply the gradient with the weights of top layers, and back propagate the gradients to the first layer.\n",
    "\n",
    "        6. After we have all the gradients, update the weight vectors using the equation w = w-lr*gradients, where lr is the learning rate and gradients is the corresponding gradient calculated for the weight vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_as_list = []\n",
    "NUM_EPOCHS = 500\n",
    "\n",
    "for EPOCH in range(NUM_EPOCHS):\n",
    "    # Forward pass: compute predicted y\n",
    "    h = x.dot(w1) # first, multiply x with w1\n",
    "    h_relu = np.maximum(h, 0) # here we remove negative values for ReLU activation.\n",
    "    y_pred = h_relu.dot(w2) # multiply h with w2 to generate the predictions.\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = np.square(y_pred - y).sum()\n",
    "    loss_as_list.append(loss)\n",
    "    print(\"Epoch: {}, Loss: {}\".format(EPOCH, loss))\n",
    "\n",
    "    # Backprop to compute gradients of w1 and w2 with respect to loss\n",
    "    grad_y_pred = 2.0 * (y_pred - y) # Note that we have to manually define the derivatives\n",
    "    grad_w2 = h_relu.T.dot(grad_y_pred) # First backprop step. \n",
    "    # Note that we are mutiplying h with output gradients to directly find the gradients of w2.\n",
    "    \n",
    "    # Finding grad_w1 requires a few additional calculations.\n",
    "    grad_h_relu = grad_y_pred.dot(w2.T) # One step in, we find gradients of hidden layer.\n",
    "    grad_h = grad_h_relu.copy()\n",
    "    grad_h[h < 0] = 0 # We apply ReLU activation.\n",
    "    grad_w1 = x.T.dot(grad_h) # and finally compute the gradients of w1.\n",
    "\n",
    "    # Update weights w1 and w2 based of equation from step 6 in our algorithm.\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2\n",
    "    \n",
    "# This will run the optimization algorithm for 500 epochs. \n",
    "# An epoch is where you traverse the entire dataset once. \n",
    "# In our case, we are sending the whole dataset at once. So each pass is one epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_as_list, 'k')\n",
    "_ = plt.title(\"Loss Curve\")\n",
    "_ = plt.xlabel(\"Epochs\")\n",
    "_ = plt.ylabel(\"Loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome! We see that our neural network learned to fit the curve well, even on random data! We'll soon look at actual MNIST code, but for now, let's understand the APIs better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pure Numpy code can be improved by using PyTorch tensors. This allows for GPU compute and easier integrations with other powerful APIs within PyTorch. We will still be doing forward and backpropagation by-hand, since we are not using `autograd` methods yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(\"YEAH\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "N, D_in, H, D_out = 64, 784, 100, 10\n",
    "\n",
    "# Create random input and output data\n",
    "# Instead of Numpy arrays, we will initialize PyTorch tensors.\n",
    "x = torch.randn(N, D_in, device=device)\n",
    "y = torch.randn(N, D_out, device=device)\n",
    "\n",
    "# Randomly initialize weights\n",
    "# Note the changes in APIs.\n",
    "w1 = torch.randn(D_in, H, device=device)\n",
    "w2 = torch.randn(H, D_out, device=device)\n",
    "\n",
    "loss_as_list = []\n",
    "learning_rate = 1e-6\n",
    "NUM_EPOCHS = 500\n",
    "\n",
    "for EPOCH in range(NUM_EPOCHS):\n",
    "    # Forward pass: compute predicted y\n",
    "    h = x.mm(w1) # We will use tensor.mm() to do matrix multiplications.\n",
    "    h_relu = h.clamp(min=0) # We can use the tensor.clamp() function to apply ReLU.\n",
    "    y_pred = h_relu.mm(w2)\n",
    "\n",
    "    # Compute and print loss; loss is a scalar, and is stored in a PyTorch Tensor\n",
    "    # of shape (); we can get its value as a Python number with loss.item().\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    loss_as_list.append(loss)\n",
    "    print(\"Epoch: {}, Loss: {}\".format(EPOCH, loss))\n",
    "\n",
    "    # Backprop to compute gradients of w1 and w2 with respect to loss\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_w2 = h_relu.t().mm(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.mm(w2.t())\n",
    "    grad_h = grad_h_relu.clone()\n",
    "    grad_h[h < 0] = 0\n",
    "    grad_w1 = x.t().mm(grad_h)\n",
    "    # Note that the equations were almost the same, and the mainly changed the APIs.\n",
    "    \n",
    "    # Update weights using gradient descent\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_as_list, 'k')\n",
    "_ = plt.title(\"Loss Curve\")\n",
    "_ = plt.xlabel(\"Epochs\")\n",
    "_ = plt.ylabel(\"Loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch Autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N, D_in, H, D_out = 64, 784, 100, 10\n",
    "\n",
    "# Create random Tensors to hold input and outputs\n",
    "x = torch.randn(N, D_in, device=device)\n",
    "y = torch.randn(N, D_out, device=device)\n",
    "\n",
    "# Create random Tensors for weights; setting requires_grad=True means that we\n",
    "# want to compute gradients for these Tensors during the backward pass.\n",
    "w1 = torch.randn(D_in, H, device=device, requires_grad=True)\n",
    "w2 = torch.randn(H, D_out, device=device, requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that `requires_grad` will enable the tensors with `grad_fn` attribute. This way, the gradients of each tensor can be found without us manually defining them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-6\n",
    "NUM_EPOCHS = 500\n",
    "loss_as_list = []\n",
    "\n",
    "for EPOCH in range(NUM_EPOCHS):\n",
    "    # Forward pass: compute predicted y using operations on Tensors. Since w1 and\n",
    "    # w2 have requires_grad=True, operations involving these Tensors will cause\n",
    "    # PyTorch to build a computational graph, allowing automatic computation of\n",
    "    # gradients. Since we are no longer implementing the backward pass by hand we\n",
    "    # don't need to keep references to intermediate values.\n",
    "    y_pred = x.mm(w1).clamp(min=0).mm(w2)\n",
    "\n",
    "    # Compute and print loss. Loss is a Tensor of shape (), and loss.item()\n",
    "    # is a Python number giving its value.\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    loss_as_list.append(loss)\n",
    "    print(\"Epoch: {}, Loss: {}\".format(EPOCH, loss))\n",
    "\n",
    "    # Use autograd to compute the backward pass. This call will compute the\n",
    "    # gradient of loss with respect to all Tensors with requires_grad=True.\n",
    "    # After this call w1.grad and w2.grad will be Tensors holding the gradient\n",
    "    # of the loss with respect to w1 and w2 respectively.\n",
    "    loss.backward()\n",
    "\n",
    "    # Update weights using gradient descent. For this step we just want to mutate\n",
    "    # the values of w1 and w2 in-place; we don't want to build up a computational\n",
    "    # graph for the update steps, so we use the torch.no_grad() context manager\n",
    "    # to prevent PyTorch from building a computational graph for the updates\n",
    "    with torch.no_grad():\n",
    "        w1 -= learning_rate * w1.grad\n",
    "        w2 -= learning_rate * w2.grad\n",
    "\n",
    "        # Manually zero the gradients after running the backward pass\n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_as_list, 'k')\n",
    "_ = plt.title(\"Loss Curve\")\n",
    "_ = plt.xlabel(\"Epochs\")\n",
    "_ = plt.ylabel(\"Loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch nn Module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch `nn` module (https://pytorch.org/docs/stable/nn.html) provides a plethora of APIs available for Linear layers, Activations, Convolution, Pooling, etc. abstracting away the manual building of the neural network architecture. In our case, we have a 3-layer neural network with one hidden-layer. We can use `nn.Linear()` to build fully-connected linear layers. ReLU activation is available as `nn.ReLU()` and does not need any attributes. Overall, a neural network can be easily built in a sequence of layers by using the `nn.Sequential()` method. Once the model is defined, we can pass it to the correct device by calling `.to(device)`, where the device is either CPU or GPU based."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N, D_in, H, D_out = 64, 784, 100, 10\n",
    "\n",
    "# Create random Tensors to hold inputs and outputs\n",
    "x = torch.randn(N, D_in, device=device)\n",
    "y = torch.randn(N, D_out, device=device)\n",
    "\n",
    "# Use the nn package to define our model as a sequence of layers. nn.Sequential\n",
    "# is a Module which contains other Modules, and applies them in sequence to\n",
    "# produce its output. Each Linear Module computes output from input using a\n",
    "# linear function, and holds internal Tensors for its weight and bias.\n",
    "# After constructing the model we use the .to() method to move it to the\n",
    "# desired device.\n",
    "model = torch.nn.Sequential(\n",
    "          torch.nn.Linear(D_in, H),\n",
    "          torch.nn.ReLU(),\n",
    "          torch.nn.Linear(H, D_out),\n",
    "        ).to(device)\n",
    "\n",
    "# The nn package also contains definitions of popular loss functions; in this\n",
    "# case we will use Mean Squared Error (MSE) as our loss function. Setting\n",
    "# reduction='sum' means that we are computing the *sum* of squared errors rather\n",
    "# than the mean; this is for consistency with the examples above where we\n",
    "# manually compute the loss, but in practice it is more common to use mean\n",
    "# squared error as a loss by setting reduction='elementwise_mean'.\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "learning_rate = 1e-4 # Note that we increased the learning rate here.\n",
    "NUM_EPOCHS = 500\n",
    "loss_as_list = []\n",
    "\n",
    "for EPOCH in range(NUM_EPOCHS):\n",
    "    # Forward pass: compute predicted y by passing x to the model. Module objects\n",
    "    # override the __call__ operator so you can call them like functions. When\n",
    "    # doing so you pass a Tensor of input data to the Module and it produces\n",
    "    # a Tensor of output data.\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # Compute and print loss. We pass Tensors containing the predicted and true\n",
    "    # values of y, and the loss function returns a Tensor containing the loss.\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    loss_as_list.append(loss)\n",
    "    print(\"Epoch: {}, Loss: {}\".format(EPOCH, loss))\n",
    "\n",
    "    # Zero the gradients before running the backward pass.\n",
    "    model.zero_grad()\n",
    "\n",
    "    # Backward pass: compute gradient of the loss with respect to all the learnable\n",
    "    # parameters of the model. Internally, the parameters of each Module are stored\n",
    "    # in Tensors with requires_grad=True, so this call will compute gradients for\n",
    "    # all learnable parameters in the model.\n",
    "    loss.backward()\n",
    "\n",
    "    # Update the weights using gradient descent. Each parameter is a Tensor, so\n",
    "    # we can access its data and gradients like we did before.\n",
    "    with torch.no_grad():\n",
    "        for param in model.parameters():\n",
    "            param.data -= learning_rate * param.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_as_list, 'k')\n",
    "_ = plt.title(\"Loss Curve\")\n",
    "_ = plt.xlabel(\"Epochs\")\n",
    "_ = plt.ylabel(\"Loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch Optim Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far in our code updating the weights have been done manually. Even when we used `autograd`, we had to manually update the weights by using `w1 -= learning_rate * w1.grad`. Torch has an `optim` package to support various optimization algorithms such as the popular Stochastic gradient descent, Adam, etc. (https://pytorch.org/docs/stable/optim.html). In this example, we shall see how we can use Adam (adaptive momentum) optimizer to automatically carry out weight updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N, D_in, H, D_out = 64, 784, 100, 10\n",
    "\n",
    "# Create random Tensors to hold inputs and outputs.\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "# Use the nn package to define our model and loss function.\n",
    "model = torch.nn.Sequential(\n",
    "          torch.nn.Linear(D_in, H),\n",
    "          torch.nn.ReLU(),\n",
    "          torch.nn.Linear(H, D_out),\n",
    "        )\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "# Use the optim package to define an Optimizer that will update the weights of\n",
    "# the model for us. Here we will use Adam; the optim package contains many other\n",
    "# optimization algorithms. The first argument to the Adam constructor tells the\n",
    "# optimizer which Tensors it should update.\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "learning_rate = 1e-4 # Note that we increased the learning rate here.\n",
    "NUM_EPOCHS = 500\n",
    "loss_as_list = []\n",
    "\n",
    "for EPOCH in range(NUM_EPOCHS):\n",
    "    # Forward pass: compute predicted y by passing x to the model.\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # Compute and print loss.\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    loss_as_list.append(loss)\n",
    "    print(\"Epoch: {}, Loss: {}\".format(EPOCH, loss))\n",
    "\n",
    "    # Before the backward pass, use the optimizer object to zero all of the\n",
    "    # gradients for the Tensors it will update (which are the learnable weights\n",
    "    # of the model)\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Backward pass: compute gradient of the loss with respect to model parameters\n",
    "    loss.backward()\n",
    "\n",
    "    # Calling the step function on an Optimizer makes an update to its parameters\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_as_list, 'k')\n",
    "_ = plt.title(\"Loss Curve\")\n",
    "_ = plt.xlabel(\"Epochs\")\n",
    "_ = plt.ylabel(\"Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
